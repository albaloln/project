{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from configparser import ConfigParser\n",
    "\n",
    "aws_access_key = 'AKIAXOQJEIM2SURHWDO5'\n",
    "aws_secret_access = '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'\n",
    "\n",
    "#Accessing the S3 buckets using boto3 client\n",
    "#s3 = boto3.resource('s3',\n",
    "#                    aws_access_key_id= parser.get('aws', 'aws_access_key'),\n",
    "#                    aws_secret_access_key=parser.get('aws', 'aws_secret_access'))\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                    aws_access_key_id= aws_access_key,\n",
    "                    aws_secret_access_key=aws_secret_access)\n",
    "\n",
    "bucket_name = 'workshop-container'\n",
    "path = 'archivo/DatafinitiElectronicsProductData.csv'\n",
    "file_name = 'DatafinitiElectronicsProductData.csv'\n",
    "\n",
    "s3.Bucket(parser.get('aws', 'bucket_name')).download_file(Key= parser.get('aws', 'path'), Filename= parser.get('aws', 'file_name'))\n",
    "#s3.Bucket(bucket_name).download_file(path, file_name)\n",
    "#df = pd.read_csv(\"DatafinitiElectronicsProductData.csv\")\n",
    " \n",
    "#print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weightconv'] = pd.to_numeric(df['weightsplit'] )\n",
    "#df['weightconv'] = pd.to_numeric(df.loc[df['measuring'] == 'oz', 'weightsplit'])\n",
    "#df['weightconv'] = [x * 16 for x in df['weightsplit', 'measuring'] if  y == 'oz' ]\n",
    "#df['weightconv'] = [x * 16 for x, y in df['weightsplit', 'measuring'] if y == 'lb']\n",
    "print(df[['measuring', 'weightsplit', 'weightconv']])\n",
    "df.to_csv ('prueba.csv')\n",
    "\n",
    "prueba =  [x  for x, y in df['weightsplit', 'measuring'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zip(df['weightsplit'], df['measuring']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weightconv'] = pd.to_numeric(df.loc[df['measuring'] == 'lb', 'weightsplit'])*16\n",
    "#df['weightconv'] = pd.to_numeric(df.loc[df['measuring'] == 'oz', 'weightsplit'])\n",
    "print(df[['measuring', 'weightsplit', 'weightconv']])\n",
    "df.to_csv ('prueba.csv')\n",
    "#import pandas as pd\n",
    "\n",
    "#df = pd.DataFrame({'A': [1, 2], 'B': [10, 20]})\n",
    "\n",
    "\n",
    "def square(x, y):\n",
    "    if x == 'lb':\n",
    "        return y * 16\n",
    "    else:\n",
    "        return y\n",
    "    \n",
    "\n",
    "\n",
    "df1['weightlb'] = df []\n",
    "\n",
    "print(df)\n",
    "print(df1)\n",
    "\n",
    "#print(df.apply(lambda row: row[\"Name\"] + \" \" + str(row[\"Percentage\"]), axis = 1))\n",
    "\n",
    "\n",
    " \n",
    "#print(df.dtypes)\n",
    "#print(df.columns)\n",
    "#print(df.head())\n",
    "#print(pd['dateAdded'])\n",
    "#print (df['weight'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto.s3.connection import S3Connection\n",
    "        \n",
    "conn = S3Connection(AWS_ACCESS_KEY,AWS_SECRET_ACCESS)\n",
    "bucket = conn.get_bucket(BUCKET_NAME)\n",
    "for key in bucket.list():\n",
    "    print (key.name.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation ():\n",
    "    #Change type of the column dateAdded from objet to datetime\n",
    "    df['dateAdded'] = pd.to_datetime(df['dateAdded'])\n",
    "    df['Year'] =  df['dateAdded'].dt.year\n",
    "    df['Month'] =  df['dateAdded'].dt.month\n",
    "    #Split column weight \n",
    "    name = df[\"weight\"].str.split(expand=True)\n",
    "    name = name.drop([2, 3], axis=1)\n",
    "    name.columns = ['weightconv', 'measuring']\n",
    "    df = pd.concat([df, name], axis=1)\n",
    "    #Homologate measuring\n",
    "    dicti={\"pounds\":\"lb\", \"lbs\":\"lb\", \"ounces\":\"oz\", \"lb\": \"lb\", \"oz\":\"oz\"}\n",
    "    df['measuring'] = df['measuring'].map(dicti)\n",
    "   \n",
    "def filterDF (df, date):\n",
    "    df2016 = df[df['dateAdded']>date] \n",
    "    return df2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import configparser\n",
    "parser = configparser.ConfigParser()\n",
    "parser.read('config.ini')\n",
    "\n",
    "AWS_ACCESS_KEY = parser.get('aws', 'aws_access_key')\n",
    "AWS_SECRET_ACCESS = parser.get('aws', 'aws_secret_access')\n",
    "BUCKET_NAME = parser.get('aws', 'bucket_name')\n",
    "PATH = parser.get('aws', 'path')\n",
    "FILE_NAME = parser.get('aws', 'file_name')\n",
    "OUTPUT_PATH = parser.get('aws', 'path')\n",
    "OUTPUT_FILE_NAME_1 = parser.get('aws', 'file_name')\n",
    "OUTPUT_FILE_NAME_2 = parser.get('aws', 'file_name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "AWS_ACCESS_KEY = 'AKIAXOQJEIM2SURHWDO5'\n",
    "AWS_SECRET_ACCESS = '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'\n",
    "BUCKET_NAME = 'workshop-container'\n",
    "PATH = 'archivo/DatafinitiElectronicsProductData.csv'\n",
    "FILE_NAME = 'DatafinitiElectronicsProductData.csv'\n",
    "OUTPUT_PATH = 'salida/AlbaMery_LondonoLondono/'\n",
    "OUTPUT_FILE_NAME = 'DatafinitiElectronicsProductData_'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Download the file in AWS\n",
    "Parameters\n",
    "----------\n",
    "bucket : name of the bucket \n",
    "path : path of the file \n",
    "file_name : name of the file in AWS \n",
    "Returns\n",
    "-------\n",
    "list: column 'weightsplit' converted to 'lb'\n",
    "\"\"\" \n",
    "def download_from_aws(bucket, path, file_name):\n",
    "    #Accessing the S3 buckets using boto3 client\n",
    "    s3 = boto3.resource('s3',\n",
    "                        aws_access_key_id= AWS_ACCESS_KEY,\n",
    "                        aws_secret_access_key= AWS_SECRET_ACCESS)\n",
    "    try:\n",
    "        #Download_file\n",
    "        s3.Bucket(bucket).download_file(path, file_name)\n",
    "        print(\"Download Successful\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(\"The file was not found\")\n",
    "        return False\n",
    "    except NoCredentialsError:\n",
    "        print(\"Credentials not available\")\n",
    "        return False\n",
    "    \n",
    "\"\"\"Return a dataframe filter by specific date \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "df : dataframe to be filtered\n",
    "date : date when dataframe will be filtered. Default '2016-11-01'\n",
    "Returns\n",
    "-------\n",
    "dfFilterAnio: filtered dataframe\n",
    "\"\"\"\n",
    "def filterDF (df, date = '2016-11-01'):\n",
    "    dfFilterAnio = df[df['dateAdded']>date] \n",
    "    dfFilterAnio.to_csv ('dfFilterAnio.csv')\n",
    "    return dfFilterAnio\n",
    "\n",
    "\"\"\"Return a list with columnn 'weightsplit' converted to 'oz', 1 lb == 16 oz\n",
    "Parameters\n",
    "----------\n",
    "df : dataframe\n",
    "date : date when dataframe will be filtered\n",
    "Returns\n",
    "-------\n",
    "list: column 'weightsplit' converted to 'lb'\n",
    "\"\"\"\n",
    "def convertLbtoOz(df):\n",
    "    dfCon = []\n",
    "    for indice_fila, fila in df.iterrows():\n",
    "        if fila['measuring'] == 'lb':\n",
    "            fila['weightoz'] = fila['weightsplit'] * 16\n",
    "        else:\n",
    "            fila['weightoz'] = fila['weightsplit']\n",
    "        dfCon.append(fila)\n",
    "    return dfCon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_from_aws (BUCKET_NAME, PATH, FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read file \n",
    "df = pd.read_csv(FILE_NAME)\n",
    "#Change type of the column dateAdded from objet to datetime\n",
    "df['dateAdded'] = pd.to_datetime(df['dateAdded'])\n",
    "df['Year'] =  df['dateAdded'].dt.year\n",
    "df['Month'] =  df['dateAdded'].dt.month\n",
    "#Split column weight \n",
    "name = df[\"weight\"].str.split(expand=True)\n",
    "name = name.drop([2, 3], axis=1)\n",
    "name.columns = ['weightsplit', 'measuring']\n",
    "df = pd.concat([df, name], axis=1)\n",
    "#Homologate measuring\n",
    "dicti={\"pounds\":\"lb\", \"lbs\":\"lb\", \"ounces\":\"oz\", \"lb\": \"lb\", \"oz\":\"oz\"}\n",
    "df['measuring'] = df['measuring'].map(dicti)\n",
    "df['weightsplit'] = pd.to_numeric(df['weightsplit'] )\n",
    "df['weightlb'] = pd.to_numeric(df['weightsplit'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffinal = pd.DataFrame(convertLbtoOz(df), columns = df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnio = filterDF(dffinal, '2016-11-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfAnio.groupby(['Year', 'Month', 'brand', 'colors', 'primaryCategories'])\\\n",
    "    .agg({'id':'size', 'reviews.rating':'mean', 'weightlb' : 'mean'})\\\n",
    "    .rename(columns={'id':'CantidadVentas','reviews.rating':'PromedioReviewsRating', 'weightlb': 'PromedioWeight'}) \\\n",
    "    .reset_index()\n",
    "df.to_csv (OUTPUT_FILE_NAME+'1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dffinal[dffinal['brand'] == 'Logitech'].groupby(['Year', 'Month']).agg({'id':'size'})\\\n",
    "    .rename(columns={'id':'CantidadVentas'}) \\\n",
    "    .reset_index() \\\n",
    "    .sort_values(by = 'CantidadVentas', ascending=False).head(3)\n",
    "    \n",
    "df2.to_csv (OUTPUT_FILE_NAME+'2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded = upload_to_aws(OUTPUT_FILE_NAME+'1.csv', BUCKET_NAME, OUTPUT_PATH+OUTPUT_FILE_NAME+'1.csv')\n",
    "uploaded = upload_to_aws(OUTPUT_FILE_NAME+'2.csv', BUCKET_NAME, OUTPUT_PATH+OUTPUT_FILE_NAME+'2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto.s3.connection import S3Connection\n",
    "\n",
    "class Aws:\n",
    "    aws_access_key = ''\n",
    "    aws_secret_access = ''\n",
    "    bucket_name = ''\n",
    "    def __init__(self, aws_access_key, aws_secret_access, bucket_name ):\n",
    "        self.aws_access_key = aws_access_key\n",
    "        self.aws_secret_access = aws_secret_access\n",
    "        self.bucket_name = bucket_name\n",
    "        \n",
    "    \"\"\"Download the file from AWS\n",
    "    Parameters\n",
    "    ----------\n",
    "    bucket : name of the bucket \n",
    "    path : path of the file \n",
    "    file_name : name of the file in AWS \n",
    "    Returns\n",
    "    -------\n",
    "    list: column 'weightsplit' converted to 'lb'\n",
    "    \"\"\" \n",
    "    def download_file(self, path, file_name, bucket):\n",
    "        #Accessing the S3 buckets using boto3 client\n",
    "        s3 = boto3.resource('s3',\n",
    "                            aws_access_key_id= self.aws_access_key,\n",
    "                            aws_secret_access_key= self.aws_secret_access)\n",
    "        try:\n",
    "            #Download_file\n",
    "            s3.Bucket(bucket).download_file(path, file_name)\n",
    "            print(\"Download Successful\")\n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            print(\"The file was not found\")\n",
    "            return False\n",
    "        except NoCredentialsError:\n",
    "            print(\"Credentials not available\")\n",
    "            return False  \n",
    "    \n",
    "    \"\"\"Upload the file to AWS\n",
    "    Parameters\n",
    "    ----------\n",
    "    local_file : name of the local file\n",
    "    bucket : name of the bucket \n",
    "    s3_file : path of the file in AWS \n",
    "    Returns\n",
    "    -------\n",
    "    boolean: True if upload was succesful and False failed \n",
    "    \"\"\"\n",
    "    def upload_file(self, local_file, bucket, file_name):\n",
    "        s3 = boto3.client('s3', aws_access_key_id= self.aws_access_key,\n",
    "                            aws_secret_access_key= self.aws_secret_access)\n",
    "        \n",
    "        try:\n",
    "            s3.upload_file(local_file, bucket, file_name)\n",
    "            print(\"Upload Successful\")\n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            print(\"The file was not found\")\n",
    "            return False\n",
    "        except NoCredentialsError:\n",
    "            print(\"Credentials not available\")\n",
    "            return False\n",
    "        \n",
    "    \"\"\"Show the file on AWS by specific bucket\n",
    "    Parameters\n",
    "    ----------\n",
    "    bucket : name of the bucket \n",
    "    \"\"\"\n",
    "    def show_file (self, bucket):\n",
    "        conn = S3Connection(self.aws_access_key,self.aws_secret_access)\n",
    "        bucket = conn.get_bucket(bucket)\n",
    "        for key in bucket.list():\n",
    "            print (key.name.encode('utf-8'))\n",
    "    \n",
    "    \"\"\"Delete file from AWS\n",
    "    Parameters\n",
    "    ----------\n",
    "    bucket : name of the bucket \n",
    "    path : path of the file \n",
    "    file_name : name of the file in AWS \n",
    "    Returns\n",
    "    -------\n",
    "    list: column 'weightsplit' converted to 'lb'\n",
    "    \"\"\" \n",
    "    def delete_file(self, path, file_name, bucket):\n",
    "        #Accessing the S3 buckets using boto3 client\n",
    "        s3 = boto3.resource('s3',\n",
    "                            aws_access_key_id= self.aws_access_key,\n",
    "                            aws_secret_access_key= self.aws_secret_access)\n",
    "        try:\n",
    "            s3.Object(bucket, path+file_name).delete()\n",
    "            print(\"Delete Successful\")\n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            print(\"The file was not found\")\n",
    "            return False\n",
    "        except NoCredentialsError:\n",
    "            print(\"Credentials not available\")\n",
    "            return False  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "AWS_ACCESS_KEY = 'AKIAXOQJEIM2SURHWDO5'\n",
    "AWS_SECRET_ACCESS = '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'\n",
    "BUCKET_NAME = 'workshop-container'\n",
    "PATH = 'archivo/DatafinitiElectronicsProductData.csv'\n",
    "FILE_NAME = 'DatafinitiElectronicsProductData.csv'\n",
    "OUTPUT_PATH = 'salida/AlbaMery_LondonoLondono/'\n",
    "OUTPUT_FILE_NAME = 'DatafinitiElectronicsProductData_'\n",
    "\n",
    "aws = Aws(AWS_ACCESS_KEY, AWS_SECRET_ACCESS, BUCKET_NAME)\n",
    "\n",
    "\"\"\"Return a dataframe filter by specific date \n",
    "\n",
    "Parameters\n",
    "----------\n",
    "df : dataframe to be filtered\n",
    "date : date when dataframe will be filtered. Default '2016-11-01'\n",
    "Returns\n",
    "-------\n",
    "dfFilterAnio: filtered dataframe\n",
    "\"\"\"\n",
    "def filterDF (df, date = '2016-11-01'):\n",
    "    dfFilterAnio = df[df['dateAdded']>date] \n",
    "    dfFilterAnio.to_csv ('dfFilterAnio.csv')\n",
    "    return dfFilterAnio\n",
    "\n",
    "\"\"\"Return a list with columnn 'weightsplit' converted to 'oz', 1 lb == 16 oz\n",
    "Parameters\n",
    "----------\n",
    "df : dataframe\n",
    "date : date when dataframe will be filtered\n",
    "Returns\n",
    "-------\n",
    "list: column 'weightsplit' converted to 'lb'\n",
    "\"\"\"\n",
    "def convertLbtoOz(df):\n",
    "    dfCon = []\n",
    "    for indice_fila, fila in df.iterrows():\n",
    "        if fila['measuring'] == 'lb':\n",
    "            fila['weightoz'] = fila['weightsplit'] * 16\n",
    "        else:\n",
    "            fila['weightoz'] = fila['weightsplit']\n",
    "        dfCon.append(fila)\n",
    "    return dfCon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "aws.download_file(PATH, FILE_NAME, BUCKET_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read file \n",
    "df = pd.read_csv(FILE_NAME)\n",
    "#Change type of the column dateAdded from objet to datetime\n",
    "df['dateAdded'] = pd.to_datetime(df['dateAdded'])\n",
    "df['Year'] =  df['dateAdded'].dt.year\n",
    "df['Month'] =  df['dateAdded'].dt.month\n",
    "#Split column weight \n",
    "name = df[\"weight\"].str.split(expand=True)\n",
    "name = name.drop([2, 3], axis=1)\n",
    "name.columns = ['weightsplit', 'measuring']\n",
    "df = pd.concat([df, name], axis=1)\n",
    "#Homologate measuring\n",
    "dicti={\"pounds\":\"lb\", \"lbs\":\"lb\", \"ounces\":\"oz\", \"lb\": \"lb\", \"oz\":\"oz\"}\n",
    "df['measuring'] = df['measuring'].map(dicti)\n",
    "df['weightsplit'] = pd.to_numeric(df['weightsplit'] )\n",
    "df['weightlb'] = pd.to_numeric(df['weightsplit'] )\n",
    "dffinal = pd.DataFrame(convertLbtoOz(df), columns = df.columns)\n",
    "\n",
    "\n",
    "dfAnio = filterDF(dffinal, '2016-11-01')\n",
    "df1 = dfAnio.groupby(['Year', 'Month', 'brand', 'colors', 'primaryCategories'])\\\n",
    "    .agg({'id':'size', 'reviews.rating':'mean', 'weightlb' : 'mean'})\\\n",
    "    .rename(columns={'id':'CantidadVentas','reviews.rating':'PromedioReviewsRating', 'weightlb': 'PromedioWeight'}) \\\n",
    "    .reset_index()\n",
    "df.to_csv (OUTPUT_FILE_NAME+'1.csv')\n",
    "\n",
    "df2 = dffinal[dffinal['brand'] == 'Logitech'].groupby(['Year', 'Month']).agg({'id':'size'})\\\n",
    "    .rename(columns={'id':'CantidadVentas'}) \\\n",
    "    .reset_index() \\\n",
    "    .sort_values(by = 'CantidadVentas', ascending=False).head(3)\n",
    "    \n",
    "df2.to_csv (OUTPUT_FILE_NAME+'2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload Successful\n",
      "Upload Successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aws.upload_file(OUTPUT_FILE_NAME+'1.csv', BUCKET_NAME, OUTPUT_PATH+OUTPUT_FILE_NAME+'1.csv')\n",
    "aws.upload_file(OUTPUT_FILE_NAME+'2.csv', BUCKET_NAME, OUTPUT_PATH+OUTPUT_FILE_NAME+'2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'/archivo/DatafinitiElectronicsProductData.csv'\n",
      "b'archivo/'\n",
      "b'archivo/DatafinitiElectronicsProductData.csv'\n",
      "b'archivo/DatafinitiElectronicsProductData.csv/'\n",
      "b'salida/'\n",
      "b'salida/AlbaMery_LondonoLondono/DatafinitiElectronicsProductData_1.csv'\n",
      "b'salida/AlbaMery_LondonoLondono/DatafinitiElectronicsProductData_2.csv'\n",
      "b'salida/AlbaMery_LondonoLondono/Docker/DatafinitiElectronicsProductData_1.csv'\n",
      "b'salida/AlbaMery_LondonoLondono/Docker/DatafinitiElectronicsProductData_2.csv'\n",
      "b'salida/Jorge_Arroyave/DatafinitiElectronicsProductData_1.csv'\n",
      "b'salida/Jorge_Arroyave/DatafinitiElectronicsProductData_2.csv'\n",
      "b'salida/Maryory_Cortes/DatafinitiElectronicsProductData_1.csv'\n",
      "b'salida/Maryory_Cortes/DatafinitiElectronicsProductData_2.csv'\n"
     ]
    }
   ],
   "source": [
    "aws.show_file(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
